---
title: "Analysis Workflow Step 3: Fit & evaluate models in outer loop"
author: "Gaylen Fronk"
format: html
---

## Setup

Load libraries
```{r, message = FALSE}
library(tidyverse)
library(tidymodels)
```

Source functions file
```{r}
source("fun_moodivate.R")
```
This functions file (fun_moodivate.R) contains many functions that are used throughout the Moodivate project analysis scripts. Functions split data, fit and evaluate models, and provide helper functionality for the modeling process. See all annotated code building functions within fun_moodivate.R.

```{r defaults}
#| include: false

knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

## Read in average metrics

```{r}
metrics_avg <- read_csv("metrics_inner_avg.csv", 
                        show_col_types = FALSE)
```

### Identify best configuration for each outer fold (i.e., across inner folds)

We select the best model configuration for each outer fold. Specifically, this is the model configuration that produces the highest average performance across *validation sets* (held-out inner folds). 
```{r best_model_2}
configs_best <- metrics_avg |> 
  group_by(outer_split_num) |> 
  arrange(desc(roc_auc)) |> 
  slice(1) |> 
  ungroup() 

configs_best |> print(n = Inf)

```

Looking at the mean and median performance of the best selected models in the inner loop, we see that there is not quite normal distribution of auROC values. Importantly, we also see evidence of optimization bias that comes from selecting and evaluating models in the same data - model performance should be centered at 0.5 (chance performance) when we have broken any relationships in the data, but performance is slightly higher. This bias motivates our use of nested cross-validation, because models are *selected* and *evaluated* using different portions of the data.

```{r}
configs_best |> pull(roc_auc) |> mean()
configs_best |> pull(roc_auc) |> median()
```

```{r}
configs_best |> 
  ggplot(aes(x = roc_auc)) +
  geom_histogram(bins = 10)
```

## Read in data

We continue to use our shuffled (i.e., randomized) dataset.

```{r}
d <- read_csv("~/Desktop/internship/moodivate/toy_data.csv", 
              show_col_types = FALSE) |> 
  glimpse()
```

## Prepare data

Following the same process as in script 01_fit_inner.qmd, we define our outcome variable (y_col_name) as `bdi_outcome`, which will be renamed as `y` to facilitate using cross-study functions and code. The two levels of the outcome variable (non-responder and responder) are set to have non-responder as the positive (event) level, as our goal is to identify non-responders to the Moodivate DMHI who should be stepped up to a higher level of care.

```{r}
y_col_name <- "bdi_outcome"
y_level_pos <- "non_responder"
y_level_neg <- "responder"
```

### Class variables & set factor levels

```{r}
d <- d |> 
  # rename outcome to y
  rename(y = !!y_col_name) |> 
  mutate(y = case_when(
    y == 0 ~ "non_responder",
    y == 1 ~ "responder",
    TRUE ~ NA_character_)) %>% 
  # y as a factor with two intuitive levels
  mutate(y = factor(y, levels = c(!!y_level_pos,
                                  !!y_level_neg))) |> 
  # move bdi_baseline to be first variable in dataset for penalty.factor
  relocate(bdi_baseline) 


```

## Fit outer loop models using best selected configurations

We follow the same model fitting process as in 01_fit_inner.qmd. The difference is that we are now fitting only the best selected model configuration in each set of held-in outer folds, and predicting into the corresponding held-out fold. 

Note that data are divided identically using the same seed.

Models are fit using `fit_predict_eval()` from fun_moodivate.R - this function fits the model and makes predictions. We map over each held-out outer fold to produce 30 *test set* estimates of model performance.

```{r eval_outer_folds}
record_ids <- d$record_id
d <- d |> 
  mutate(id_obs = record_id)
rm(record_ids)

cv_resample_type <- "nested"
cv_outer_resample <- "3_x_10"
cv_inner_resample <- "1_x_10"
seed_splits <- 52592

splits <- d |> 
  make_splits(cv_resample_type, cv_resample, cv_outer_resample,
              cv_inner_resample, the_seed = seed_splits)

all <- configs_best$outer_split_num |> 
  map(\(split_num) fit_predict_eval(split_num, splits, configs_best))  

metrics_out <- all |> 
  map(\(l) pluck(l, "metrics_out")) |> 
  list_rbind() |> 
  write_rds("outer_metrics.rds")

preds_out <- all |> 
  map(\(l) pluck(l, "probs_out")) |> 
  list_rbind() |> 
  write_rds("outer_preds.rds")
```

## Evaluate performance

### Inner Loop AUC

Best model configurations were selected using the median auROCs across 10 inner folds. 30 (3x10) models were selected. These performance estimates are from the inner folds (i.e., validation sets) and were used *only* for selection and not for evaluation.

```{r}
metrics_out |> 
  summarize(median(roc_auc_in), mean(roc_auc_in), 
            min(roc_auc_in), max(roc_auc_in), sd(roc_auc_in)) |> 
  glimpse()
```

### Outer AUC

Best model configurations were evaluated using the auROCs from the 30 (3x10) outer folds (i.e., test sets). These performance metrics were used *only* for evaluation and not for selection. 

Outer overall
```{r metrics_out}
metrics_out |> 
  summarize(median(roc_auc), mean(roc_auc),
            min(roc_auc), max(roc_auc), sd(roc_auc)) |> 
  glimpse()
```

Side by side of inner & outer median AUCs
```{r}
metrics_out |> 
  summarize(median(roc_auc), median(roc_auc_in),
            mean(roc_auc), mean(roc_auc_in)) |> 
  glimpse()
```

Plot outer fold auROCs
```{r plot_outer}
metrics_out |> 
  ggplot(aes(x = roc_auc)) +
  geom_histogram(bins = 10) 

```

### ROC curve

This is single auROC by concatenating all outer folds.
Could consider reporting this auROC though likely average of outer fold auROCs is more appropriate (and/or Bayesian posterior probability distributions). 

```{r roc_info}
preds_out %>%
  roc_auc(prob_raw, truth = label)

roc_data <- preds_out %>%
  roc_curve(prob_raw, truth = label)

roc_data %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .threshold)) +
  geom_path(linewidth = 2) +
  geom_abline(lty = 3) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "False Positive Rate",
       y = "True Positive Rate",
       color = "Threshold") +
  scale_x_continuous(breaks = seq(0,1,.25),
    labels = sprintf("%.2f", seq(1,0,-.25))) +
  scale_color_gradient(low="blue", high="red") +
  theme(axis.text = element_text(size = rel(1.50)),
        axis.title = element_text(size = rel(1.75)))
```

Add individual outer fold auROC curves for visualization.
```{r}

# rocs per fold
roc_folds <- preds_out %>%
  nest(.by = outer_split_num, .key = "preds") |> 
  mutate(roc = map(preds, \(preds) roc_curve(preds, prob_raw, 
                                             truth = label)))

fig_roc_folds <- roc_data %>%  # plot region from full concatenated data 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_abline(lty = 3) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "False Positive Rate",
       y = "True Positive Rate") +
  scale_x_continuous(breaks = seq(0,1,.25),
                     labels = sprintf("%.2f", seq(1,0,-.25))) +
  theme(axis.text = element_text(size = rel(1.25)),
        axis.title = element_text(size = rel(1.5)))

for (i in 1:nrow(roc_folds)) {
  fig_roc_folds <- fig_roc_folds +
    geom_path(data = roc_folds$roc[[i]],
              mapping = aes(x = 1 - specificity, y = sensitivity),
              color = "gray")
}

#add full concatenated curve
fig_roc_folds +
  geom_path(data = roc_data,
            mapping = aes(x = 1 - specificity, y = sensitivity, 
                          color = .threshold),
            linewidth = 2) +
  scale_color_gradient(low="blue", high="red") +
  labs(color = "Threshold",
       x = "False Positive Rate")
```