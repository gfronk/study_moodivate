---
title: "Analysis Workflow Step 1: Fit and select in inner loops"
author: "Gaylen Fronk"
format: html
---

## Setup

Load libraries
```{r, message = FALSE}
library(glmnet)
library(tidyverse)
library(tidymodels)
```

Source functions file
```{r}
source("fun_moodivate.R")
```
This functions file (fun_moodivate.R) contains many functions that are used throughout the Moodivate project analysis scripts. Functions split data, fit and evaluate models, and provide helper functionality for the modeling process. See all code building functions within fun_moodivate.R. 

## Read in data

To ensure all proposed analyses are feasible and to specify analysis steps as precisely as possible, **we conduct our proposed analyses using a shuffled (i.e., randomized) version of our dataset**. Following testing, our analyses will follow these scripts exactly using our real data.

To create our shuffled dataset, we randomly sample (without replacement) **EVAN: IS THIS TRUE?** the outcome variable (`bdi_outcome`, which will later be renamed to `y`). This process:

* Breaks any relationship between the outcome and predictor variables

* Maintains any possible relationships among predictor variables

* Maintains the exact data structure (variable types, outcome variable distribution, predictor variable distributions)

```{r}
d <- read_csv("~/Desktop/internship/moodivate/toy_data.csv", 
              show_col_types = FALSE) |> 
  glimpse()
```

## Prepare data

### Set up outcome variable levels

We define our outcome variable (y_col_name) as `bdi_outcome`, which will be renamed as `y` to facilitate using cross-study functions and code. The two levels of the outcome variable (non-responder and responder) are set to have non-responder as the positive (event) level, as our goal is to identify non-responders to the Moodivate DMHI who should be stepped up to a higher level of care.
```{r}
y_col_name <- "bdi_outcome"
y_level_pos <- "non_responder"
y_level_neg <- "responder"
```

### Class variables & set factor levels

```{r}
d <- d |> 
  # rename outcome to y
  rename(y = !!y_col_name) |> 
  mutate(y = case_when(
                       y == 0 ~ "non_responder",
                       y == 1 ~ "responder",
                       TRUE ~ NA_character_)) %>% 
  # y as a factor with two levels, positive level first
  mutate(y = factor(y, levels = c(!!y_level_pos,
                                  !!y_level_neg))) |> 
  # standardize naming
  rename_with(~ str_replace(.x, "_w1", "_wk1")) |> 
  rename_with(~ str_replace(.x, "_w2", "_wk2")) |> 
  rename_with(~ str_replace(.x, "_w3", "_wk3")) |> 
  rename_with(~ str_replace(.x, "_w4", "_wk4")) |> 
  # move bdi_baseline to be first variable in dataset for penalty.factor
  relocate(bdi_baseline) 


```

## Divide data

### Set cross-validation parameters

We are using nested cross-validation, with three repeats of 10-fold CV in the outer loop and one repeat of 10-fold CV in the inner loop. We set a seed for replicability across scripts and across model runs.

For more details on nested cross-validation, see **GEF ADD KRSTAJIC PUB LINK HERE**. 
```{r}
cv_resample_type <- "nested"
cv_outer_resample <- "3_x_10"
cv_inner_resample <- "1_x_10"
seed_splits <- 52592
```

### Make splits 

Divide data using `make_splits()` function (from fun_moodivate.R). We are stratifying by the outcome variable `y` to ensure equal distribution of outcome classes across all inner and outer folds. 
```{r}
splits <- d |> 
  make_splits(cv_resample_type, cv_resample, cv_outer_resample,
              cv_inner_resample, the_seed = seed_splits,
              strata = "y")
```

## Set up model configurations

We define our statistical algorithm, "glmnet" (elastic net regression), and "ml_mode" (classification vs. regression). We define ranges for the two glmnet tuning parameters, alpha (mixture, `hp1`) and lambda (penalty, `hp2`). 

```{r}
algorithm <- "glmnet"
ml_mode <- "classification"

# alpha (mixture)
hp1_glmnet <- c(0, seq(.1, 1, length.out = 10)) 

# lambda (penalty)
hp2_glmnet_min <- -8 # min for penalty grid - will be passed into exp(seq(min, max, length.out = out))
hp2_glmnet_max <- 2 # max for penalty grid
hp2_glmnet_out <- 100 # length of penalty grid
```

## Make configurations tibble

This is a grid expansion of the splits, hyperparameters, and feature sets. Each row in the tibble will serve as a model configuration that can then be fit below. This tibble will also be used to connect results to model configurations.

### Extract CV parameters from strings

```{r}
# outer cv loop
outer_n_repeats <- as.numeric(str_remove(cv_outer_resample,
                                         "_x_\\d{1,2}"))
outer_n_folds <- as.numeric(str_remove(cv_outer_resample,
                                       "\\d{1,3}_x_"))
outer_split_num <- 1:(outer_n_repeats * outer_n_folds)

# inner cv loop
inner_n_repeats <- as.numeric(str_remove(cv_inner_resample,
                                         "_x_\\d{1,2}"))
inner_n_folds <- as.numeric(str_remove(cv_inner_resample,
                                       "\\d{1,3}_x_"))
inner_split_num <- 1:(inner_n_repeats * inner_n_folds)

```

### Make configurations grid

```{r}
configs <- expand_grid(outer_split_num = outer_split_num,
                       inner_split_num = inner_split_num,
                       feature_set = c("thru_wk2", "thru_wk3", "thru_wk4"),
                       hp1 = hp1_glmnet,
                       hp2 = NA_integer_) |> 
  tibble::rownames_to_column("config_num") |> 
  mutate(config_num = as.numeric(config_num))

glimpse(configs)
```

Feature sets are specific combinations of features. Here, we are fitting models separately that use data from weeks 1 & 2, weeks 1 through 3, and weeks 1 through 4 so that we can examine the trade-off between model performance and duration of data collection required before making a decision. 

Note that hp2 (lambda/penalty) is listed as NA because each configuration will `tune()` across the 100 values set using `hp2_glmnet_min`, `hp2_glmnet_max`, and `hp2_glmnet_out` (length). 

## Fit models

Set up wrapper function to `map()` over every model configuration (i.e., every row in the configs tibble). For each model configuration, we:

1. Filter down to only that configuration's row in the model configurations tibble.

2. Build a recipe using the custom `build_recipe()` function (in fun_moodivate.R). This function defines a recipe, which is a set of processing steps to convert the raw data into the features that will serve as model inputs. Our recipe always contains the following generic steps: regress outcome variable `y` on all other variables, remove the `record_id` variable, standardize all variables to have a mean of 0 and standard deviation of 1 (required for the glmnet algorithm), and remove any near-zero-variance variables. Additionally, the recipe is adjusted based on the configuration's `feature_set` such that week 3 and week 4 data are removed as indicated. It will be developed using held-in data and applied to held-out data (e.g., standardization values will be derived from held-in data).

3. Define a vector of penalty weights, which will apply equal penalty weighting to all variables except `bdi_baseline`, which will have a penalty weight of 0 and consequently will be included in all models. The length of the `penalty_weights` vector is determined by the number of features in the feature set, so we temporarily build a feature set using our recipe (from step 2) to obtain the number of columns. We use the total number of columns minus 2 (1 for `bdi_baseline`, which already has a penalty weight of 1; and 1 for `y`, which is in the feature set but not counted as a predictor variable). 

4. Fit model, and get model performance metrics using our custom `tune_model()` from the fun_moodivate.R script. This function performs model tuning for the model configuration (defined by `feature_set`, `hp1`, and combination of inner and outer folds) across our range of values of `hp2`. 

5. Append model performance metrics to the configs tibble in a new `results` tibble.

```{r}
fit_eval <- function(config_current, configs, d, splits) {
  
  # filter single config row from configs
  config <- configs |> 
    filter(config_num == config_current)
  
  # build recipe
  rec <- build_recipe(d = d, config = config)
  
  # create penalty_weights vector using feature set dimensions
  feat <- rec |> 
    prep(training = d, strings_as_factors = FALSE) |> 
    bake(new_data = NULL) 
  penalty_weights <- c(0, rep(1, ncol(feat) - 2))
  rm(feat) 
  
  # fit model & get predictions and model metrics
  results <- tune_model(config = config,
                        rec = rec, 
                        splits = splits, 
                        ml_mode = ml_mode, 
                        cv_resample_type = cv_resample_type,
                        hp2_glmnet_min = hp2_glmnet_min, 
                        hp2_glmnet_max = hp2_glmnet_max, 
                        hp2_glmnet_out = hp2_glmnet_out,
                        y_level_pos = y_level_pos,
                        penalty_weights = penalty_weights)
  
  return(results)
}

```

### Fit models across all configurations

```{r}
if (file.exists("results.csv")) {
  results <- read_csv("results.csv", show_col_types = F)
} else {
  results <- 1:max(configs$config_num) |> 
  map(\(config_current) fit_eval(config_current, configs, 
                                 d, splits)) |> 
  list_rbind() %>% 
  mutate(new_config_num = 1:nrow(.))
}

```

### Glimpse results

This results file contains the information from the configuration grid, with model performance metrics appended to each row (i.e., model configuration). The grid has now been expanded to include each individual value for `hp2` (lambda/penalty) that was considered within `tune_models()`. 

```{r}
glimpse(results)
```

Save results
```{r}
results |> 
  write_csv("results.csv")
```


