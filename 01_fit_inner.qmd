---
title: "Analysis Workflow Step 1: Fit and select in inner loops"
author: "Gaylen Fronk"
format: html
---

## Setup

Load libraries
```{r, message = FALSE}
library(glmnet)
library(tidyverse)
library(tidymodels)
```

Source functions file
```{r}
source("fun_moodivate.R")
```
This functions file (fun_moodivate.R) contains many functions that are used throughout the Moodivate project analysis scripts. Functions split data, fit and evaluate models, and provide helper functionality for the modeling process. See all code building functions within fun_moodivate.R. 

## Read in data

To ensure all proposed analyses are feasible and to specify analysis steps as precisely as possible, **we conduct our proposed analyses using a shuffled (i.e., randomized) version of our dataset**. Following testing, our analyses will follow these scripts exactly using our real data.

To create our shuffled dataset, we randomly sample (without replacement) **EVAN: IS THIS TRUE?** the outcome variable (`bdi_outcome`, which will later be renamed to `y`). This process:

* Breaks any relationship between the outcome and predictor variables

* Maintains any possible relationships between predictor variables

* Maintains the exact data structure (variable types, outcome variable distribution, predictor variable distributions)

```{r}
d <- read_csv("~/Desktop/internship/moodivate/toy_data.csv", 
              show_col_types = FALSE) |> 
  glimpse()
```

## Prepare data

### Set up outcome variable levels

We define our outcome variable (y_col_name) as `bdi_outcome`, which will be renamed as `y` to facilitate using cross-study functions and code. The two levels of the outcome variable (non-responder and responder) are set to have non-responder as the positive (event) level, as our goal is to identify non-responders to the Moodivate DMHI who should be stepped up to a higher level of care.
```{r}
y_col_name <- "bdi_outcome"
y_level_pos <- "non_responder"
y_level_neg <- "responder"
```

### Class variables & set factor levels

```{r}
d <- d |> 
  # rename outcome to y
  rename(y = !!y_col_name) |> 
  mutate(y = case_when(
                       y == 0 ~ "non_responder",
                       y == 1 ~ "responder",
                       TRUE ~ NA_character_)) %>% 
  # y as a factor with two levels, positive level first
  mutate(y = factor(y, levels = c(!!y_level_pos,
                                  !!y_level_neg))) |> 
  # move bdi_baseline to be first variable in dataset for penalty.factor
  relocate(bdi_baseline) 


```

## Divide data

### Set cross-validation parameters

We are using nested cross-validation, with three repeats of 10-fold CV in the outer loop and one repeat of 10-fold CV in the inner loop. We set a seed for replicability across scripts and across model runs.

For more details on nested cross-validation, see **GEF ADD KRSTAJIC PUB LINK HERE**. 
```{r}
cv_resample_type <- "nested"
cv_outer_resample <- "3_x_10"
cv_inner_resample <- "1_x_10"
seed_splits <- 52592
```

### Make splits 

Divide data using `make_splits()` function (from fun_moodivate.R). We are stratifying by the outcome variable `y` to ensure equal distribution of outcome classes across inner and outer folds. 
```{r}
splits <- d |> 
  make_splits(cv_resample_type, cv_resample, cv_outer_resample,
              cv_inner_resample, the_seed = seed_splits,
              strata = "y")
```

## Set up model configurations & build recipe

### Set model configuration parameters

We select our statistical algorithm, glmnet (elastic net regression), and "ml_mode" (classification vs. regression). 

We define ranges for the two glmnet tuning parameters, alpha (mixture) and lambda (penalty). We also set a vector of penalty weights, which will apply equal penalty weighting to all variables except `bdi_baseline`, which will have a penalty weight of 0 and consequently will be included in all models.
```{r}
algorithm <- "glmnet"
ml_mode <- "classification"

# alpha (mixture)
hp1_glmnet <- c(0, seq(.1, 1, length.out = 10)) 

# lambda (penalty)
hp2_glmnet_min <- -8 # min for penalty grid - will be passed into exp(seq(min, max, length.out = out))
hp2_glmnet_max <- 2 # max for penalty grid
hp2_glmnet_out <- 100 # length of penalty grid

penalty_weights <- c(0, rep(1, ncol(d) - 3))
# subtracting 3 for: 1) bdi_baseline (which now has penalty weight of 0, first feature in dataset), 2) bdi_outcome (y), 3) record_id that gets removed in recipe
```

### Build recipe

The `tidymodels` ecosystem uses recipes to convert predictor variables into feature sets used for model inputs. We remove record_id (not a predictor variable, only a participant identifier), standardize all features (required for glmnet for equal weighting of features), and remove near-zero variance features. 

This recipe will be used in all model fitting. It will be developed using held-in data and applied to held-out data (e.g., standardization values will be derived from held-in data). 
```{r}
rec <- recipe(y ~ ., data = d) |> 
  step_rm(record_id) |> 
  # standardize features to have M=0, SD=1, required for glmnet for weighting
  step_normalize(all_predictors()) |> 
  # remove near-zero-variance features
  step_nzv(all_predictors())
```

## Make configurations tibble

This is a grid expansion of the splits, hyperparameters, and feature sets. Each row in the tibble will serve as a model configuration that can then be fit below. This tibble will also be used to connect results to model configurations.

### Extract CV parameters from strings

```{r}
# outer cv loop
outer_n_repeats <- as.numeric(str_remove(cv_outer_resample,
                                         "_x_\\d{1,2}"))
outer_n_folds <- as.numeric(str_remove(cv_outer_resample,
                                       "\\d{1,3}_x_"))
outer_split_num <- 1:(outer_n_repeats * outer_n_folds)

# inner cv loop
inner_n_repeats <- as.numeric(str_remove(cv_inner_resample,
                                         "_x_\\d{1,2}"))
inner_n_folds <- as.numeric(str_remove(cv_inner_resample,
                                       "\\d{1,3}_x_"))
inner_split_num <- 1:(inner_n_repeats * inner_n_folds)

```

### Make configurations grid

```{r}
configs <- expand_grid(outer_split_num = outer_split_num,
                       inner_split_num = inner_split_num,
                       feature_set = c("thru_wk2", "thru_wk3", "thru_wk4"),
                       hp1 = hp1_glmnet,
                       hp2 = NA_integer_) |> 
  tibble::rownames_to_column("config_num") |> 
  mutate(config_num = as.numeric(config_num))

glimpse(configs)
```

Feature sets are specific combinations of features. Here, we are fitting models separately that use data from weeks 1 & 2, weeks 1 through 3, and weeks 1 through 4 so that we can examine the trade-off between model performance and duration of data collection required before making a decision. 

Note that hp2 (lambda/penalty) is listed as NA because each configuration will `tune()` across the 100 values set using `hp2_glmnet_min`, `hp2_glmnet_max`, and `hp2_glmnet_out` (length). 

## Fit models

Set up wrapper function that contains `tune_model()` from fun_moodivate.R. This wrapper will allow us to `map()` over every model configuration (i.e., every row in the configs tibble). 
```{r}
fit_eval <- function(config_current, configs, d, splits) {
  
  # filter single config row from configs
  config <- configs |> 
    filter(config_num == config_current)
  
  # fit model & get predictions and model metrics
  results <- tune_model(config = config,
                        rec = rec, 
                        splits = splits, 
                        ml_mode = ml_mode, 
                        cv_resample_type = cv_resample_type,
                        hp2_glmnet_min = hp2_glmnet_min, 
                        hp2_glmnet_max = hp2_glmnet_max, 
                        hp2_glmnet_out = hp2_glmnet_out,
                        y_level_pos = y_level_pos,
                        penalty_weights = penalty_weights)
  
  return(results)
}

```

### Fit models across all configurations

```{r}
if (file.exists("results.csv")) {
  results <- read_csv("results.csv", show_col_types = F)
} else {
  results <- 1:max(configs$config_num) |> 
  map(\(config_current) fit_eval(config_current, configs, 
                                 d, splits)) |> 
  list_rbind() %>% 
  mutate(new_config_num = 1:nrow(.))
}

```

### Glimpse results

This results file contains the information from the configuration grid, with model performance metrics appended to each row (i.e., model configuration). The grid has now been expanded to include each individual value for `hp2` (lambda/penalty) that was considered within `tune_models()`. 

```{r}
glimpse(results)
```

Save results
```{r}
results |> 
  write_csv("results.csv")
```


